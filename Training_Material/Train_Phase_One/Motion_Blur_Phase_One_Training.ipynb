{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRPN2qHyXzDviLO2+tTRef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hincz-lab/motion-blur-microscopy/blob/main/Training_Material/Train_Phase_One/Motion_Blur_Phase_One_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "ncTWIPUpRKeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will be used to train the phase one segmentation network for use on MBM images and videos. To begin, let's first clone a Github directory to work in.\n",
        "\n",
        "You will need to go into the next block of code by double clicking the \"Show Code\" button underneath the \"Clone Github Repository\" header. Please replace the code <token> with your personal access token. \n",
        "\n",
        "To generate a personal access token, in Github, click on your icon at the top right and choose settings, then click on developer settings, and finally, create a personal access token. So, its settings -> Developer Settings -> Personal Access Token (Classic).\n",
        "\n",
        "After cloning the repository, you can see the repository in the \"Files\" section of Colab, located on the left-hand side of the screen."
      ],
      "metadata": {
        "id": "A-1PxGqIRNsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "c1mobHYWRJ23",
        "outputId": "82f39f4d-3f2c-4db9-829c-47c09a02a52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'motion-blur-microscopy'...\n",
            "remote: Enumerating objects: 415, done.\u001b[K\n",
            "remote: Counting objects: 100% (270/270), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 415 (delta 119), reused 138 (delta 59), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (415/415), 45.23 MiB | 21.54 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Clone Github Repository\n",
        "!git clone https://<token>@github.com/hincz-lab/motion-blur-microscopy.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we import any libraries or packages we may need for the rest of this document."
      ],
      "metadata": {
        "id": "0Fp54tQ_VXBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Packages And Libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import load_model, Model, model_from_json\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras import activations \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os as os\n",
        "import shutil\n",
        "import cv2 as cv2\n",
        "\n",
        "from random import randint\n",
        "from random import random"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ynwA7hNAVbY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "V744w5XdXb7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by first importing all of the MBM training images and their respective labeled and layered masks. You can import the MBM training images to the directory **motion-blur-microscopy -> Training_Material -> Train_Phase_One -> Original_Tiles**. You can import all of the corresponding labeled and layered masks to the directory **motion-blur-microscopy -> Training_Material -> Train_Phase_One -> Labeled_Layered_Tiles**.\n",
        "\n",
        "Note, that MBM training images and their respective labeled and layered masks should have the same name in both directories. The MBM training images should be .png files, and the labeled and layered masks should be .npy files. Furthermore, the MBM training images and labeled and layered masks should be the resized version (128x128).\n",
        "\n",
        "Once you have uploaded your MBM training images and labeled and layered masks, you can run the next block of code, which will complete two tasks.\n",
        "\n",
        "\n",
        "1.   The code will move all of the uploaded documents to a seperate directory at **motion-blur-microscopy -> Training_Material -> Train_Phase_One -> Flow_Folder**\n",
        "2.   The code will create a .csv document, which will contain the names of all of the MBM training images and their respective labeled and layered masks. \n",
        "\n",
        "Both the flow folder and the .csv document are necessary for the training of the phase one segmentation network.\n",
        "\n"
      ],
      "metadata": {
        "id": "B9W72GdgXeHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prepare Data For training\n",
        "\n",
        "base_Directory = \"motion-blur-microscopy/Training_Material/Train_Phase_One/\"\n",
        "\n",
        "for subdirectory in os.listdir(base_Directory):\n",
        "  if '.ipynb_checkpoints' in os.listdir(base_Directory + subdirectory):\n",
        "    os.rmdir(base_Directory + subdirectory + \"/.ipynb_checkpoints\")\n",
        "  if 'blank.txt' in os.listdir(base_Directory + subdirectory):\n",
        "      os.remove(base_Directory + subdirectory + \"/blank.txt\")\n",
        "\n",
        "image_Names = os.listdir(base_Directory + \"Original_Tiles/\")\n",
        "mask_Names = os.listdir(base_Directory + \"Labeled_Layered_Tiles/\")\n",
        "flow_Directory = base_Directory + \"Flow_Folder\"\n",
        "data_Frame_Location = base_Directory + \"Excel_Directory/Training_File_Names.csv\"\n",
        "\n",
        "for file in os.listdir(base_Directory + \"Original_Tiles/\"):\n",
        "  shutil.copy(base_Directory + \"Original_Tiles/\" + file, base_Directory + \"Flow_Folder/\" + file)\n",
        "\n",
        "for file in os.listdir(base_Directory + \"Labeled_Layered_Tiles/\"):\n",
        "  shutil.copy(base_Directory + \"Labeled_Layered_Tiles/\" + file, base_Directory + \"Flow_Folder/\" + file)\n",
        "\n",
        "dict = {'X': np.sort(image_Names), 'Y_True': np.sort(mask_Names)} \n",
        "training_Data_Frame = pd.DataFrame(dict)\n",
        "training_Data_Frame.to_csv(data_Frame_Location)"
      ],
      "metadata": {
        "id": "SxqjD9CMV_qI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "n_9XtUpoqQdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the training, we will first establish the architecture of our segmentation network by running the following block of code."
      ],
      "metadata": {
        "id": "UpR8zjCLqSpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Architecture\n",
        "\n",
        "# ================= Phase 1 Models ====================\n",
        "\n",
        "def Phase1_Net(img_size, num_classes):\n",
        "    inputs = Input(shape=img_size + (3,))\n",
        "\n",
        "    x = Conv2D(64,kernel_size = 3, strides = (1,1),\n",
        "                            padding = \"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "    \n",
        "    previous_block_concatenate1 = x\n",
        "    x = MaxPooling2D(pool_size = (2,2),\n",
        "                                  strides = (2,2))(x)\n",
        "\n",
        "    x = Conv2D(128,kernel_size = 3, strides = (1,1),\n",
        "                            padding = \"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "    x = MaxPooling2D(pool_size = (2,2),\n",
        "                                  strides = (2,2))(x)\n",
        "\n",
        "    previous_block_concatenate2 = x\n",
        "\n",
        "    concate_block_num = 3\n",
        "    for filters in [256, 512, 512]:\n",
        "        x = Conv2D(filters,3, strides = (1,1),\n",
        "                            padding = \"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(activations.relu)(x)\n",
        "        x = Conv2D(filters,3, strides = 1,\n",
        "                         padding = \"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(activations.relu)(x)\n",
        "        x = MaxPooling2D(pool_size = (2,2),\n",
        "                                  strides = (2,2))(x)\n",
        "        globals()['previous_block_concatenate%s' % concate_block_num] = x\n",
        "        concate_block_num = concate_block_num + 1\n",
        "        print((\"No errors for filter size:\" + str(filters)))\n",
        "\n",
        "\n",
        "\n",
        "    x = Conv2D(512,3, strides = 1,\n",
        "                            padding = \"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "    x = MaxPooling2D(pool_size = (2,2),\n",
        "                                  strides = (2,2))(x)\n",
        "\n",
        "    x = Conv2D(512,3, strides = 1,\n",
        "                            padding = \"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "    x = Conv2DTranspose(256,2, strides = (2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "    x = concatenate([x, previous_block_concatenate5], axis =-1)\n",
        "\n",
        "    x = Conv2DTranspose(256,2, strides = (2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "    x = concatenate([x, previous_block_concatenate4],axis=-1)\n",
        "\n",
        "    x = Conv2DTranspose(128,2, strides = (2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "    x = concatenate([x, previous_block_concatenate3],axis=-1)\n",
        "    \n",
        "    x = Conv2DTranspose(64,2, strides = (2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "    x = concatenate([x, previous_block_concatenate2],axis=-1)\n",
        "\n",
        "\n",
        "    x = Conv2DTranspose(32,2, strides = (2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "    x = Conv2DTranspose(64,2, strides = (2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "\n",
        "\n",
        "    x = concatenate([x, previous_block_concatenate1],axis=-1)\n",
        "\n",
        "    x = Conv2D(32,3, strides = (1,1),\n",
        "                            padding = 'same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "    x = Conv2D(num_classes,3, strides = (1,1),\n",
        "                            padding = 'same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activations.relu)(x)\n",
        "    outputs = Conv2D(num_classes,3, strides = (1,1),\n",
        "                            activation = 'softmax',\n",
        "                            padding = 'same',\n",
        "                            name = 'sRBC_classes')(x)\n",
        "    model = Model(inputs,outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ================ Train Phase 2 Model ================\n",
        "\n",
        "# training the model for a specific amount of epochs \n",
        "def Phase1_train_network(model, X_train, y_train, \n",
        "                        X_test, y_test, epochs):\n",
        "    \n",
        "    train_history = model.fit(X_train, y_train, epochs=epochs, \n",
        "                              validation_data=(X_test, y_test),\n",
        "                              shuffle = True, verbose = 2)\n",
        "    return model, train_history"
      ],
      "metadata": {
        "cellView": "form",
        "id": "73CwQK69qZ0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create a data generator, which will read MBM training images and their labeled and layered masks in in batches, which will reduce memory requirements."
      ],
      "metadata": {
        "id": "-EWzhei7rMLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Data Generator\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, data_Frame, x_Col, y_Col, directory,tile_Namesake, mask_Namesake, subset = None,\n",
        "                 horizontal_Flips = False, vertical_Flips = False, rotations = False, batch_Size = 32,\n",
        "                 split = False, training_Ratio = 1, shuffle = False, dim = (128,128), number_Of_Channels = 3,\n",
        "                 number_Of_Classes = 2, sample_Mean_Zero_Center_Standarardization = True, number_Of_Training_Images = None):\n",
        "        self.batch_Size = batch_Size\n",
        "        self.df = data_Frame\n",
        "        self.x_Col = x_Col\n",
        "        self.y_Col = y_Col\n",
        "        self.dim = dim\n",
        "        self.directory = directory\n",
        "        self.subset = subset\n",
        "        self.sample_Mean_Zero_Center_Standarardization = sample_Mean_Zero_Center_Standarardization\n",
        "        self.number_Of_Classes = number_Of_Classes\n",
        "        self.number_Of_Channels = number_Of_Channels\n",
        "        self.shuffle = shuffle\n",
        "        self.tile_Names = self.df[self.x_Col]\n",
        "        self.truth_Names = self.df[self.y_Col]\n",
        "        self.tile_Namesake = tile_Namesake\n",
        "        self.mask_Namesake = mask_Namesake\n",
        "        self.horizontal_Flips = horizontal_Flips\n",
        "        self.vertical_Flips = vertical_Flips\n",
        "        self.number_Of_Training_Images = number_Of_Training_Images\n",
        "        self.index_List = np.arange(number_Of_Training_Images) + 1\n",
        "        self.rotations = rotations\n",
        "        self.training_Samples = int(training_Ratio*len(self.index_List))\n",
        "        if split == True:\n",
        "            self.train_Index_List = self.index_List[:self.training_Samples]\n",
        "            self.validate_Index_List = self.index_List[self.training_Samples:]\n",
        "        else:\n",
        "            self.train_Index_List = self.index_List[:]\n",
        "            self.validate_Index_List = []\n",
        "        if self.shuffle == True:\n",
        "            self.on_Epoch_End()\n",
        "    def __len__(self):\n",
        "        return int(len(self.train_Index_List)/self.batch_Size)\n",
        "    def __getitem__(self, index):\n",
        "        if self.subset == \"Training\":\n",
        "            indexes = self.train_Index_List[index*self.batch_Size:(index*self.batch_Size) + self.batch_Size]\n",
        "            X, y_True = self.generate_Batch(indexes)\n",
        "        elif self.subset == \"Validation\":\n",
        "            indexes = self.validate_Index_List[index*self.batch_Size:(index*self.batch_Size) + self.batch_Size]\n",
        "            X, y_True = self.generate_Batch(indexes)\n",
        "        else:\n",
        "            indexes = self.train_Index_List[index*self.batch_Size:(index*self.batch_Size) + self.batch_Size]\n",
        "            X, y_True = self.generate_Batch(indexes)\n",
        "        return X, y_True\n",
        "    def on_Epoch_End(self):\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.train_Index_List)\n",
        "    def generate_Batch(self,indexes):\n",
        "        X = np.zeros((self.batch_Size, *self.dim, self.number_Of_Channels))\n",
        "        y_True = np.zeros((self.batch_Size, *self.dim, self.number_Of_Classes))\n",
        "        for index in range(len(indexes)):\n",
        "            if self.sample_Mean_Zero_Center_Standarardization == True:\n",
        "                img = plt.imread(self.directory + self.tile_Namesake + str(indexes[index]) + \".png\")[:,:,0:3]\n",
        "                if np.max(img) == int(np.max(img)) and len(str(np.max(img))) == len(str(int(np.max(img)))):\n",
        "                    img = img.copy()/255.\n",
        "                if len(np.shape(img)) == 2:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "                if np.shape(img)[2] == 4:\n",
        "                    img = img.copy()[:,:,0:3]\n",
        "                mask = np.load(self.directory + self.mask_Namesake + str(indexes[index]) + \".npy\")\n",
        "                img, mask = self.augment_Image(img,mask)\n",
        "                X[index,:,:,:] = self.standard_norm(plt.imread(self.directory + self.tile_Namesake + str(indexes[index]) + \".png\")[:,:,0:3])\n",
        "                y_True[index,:,:,:] = np.load(self.directory + self.mask_Namesake + str(indexes[index]) + \".npy\")\n",
        "            else:\n",
        "                img = plt.imread(self.directory + self.tile_Namesake + str(indexes[index]) + \".png\")[:,:,0:3]\n",
        "                if np.max(img) == int(np.max(img)) and len(str(np.max(img))) == len(str(int(np.max(img)))):\n",
        "                    img = img.copy()/255.\n",
        "                if len(np.shape(img)) == 2:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "                if np.shape(img)[2] == 4:\n",
        "                    img = img.copy()[:,:,0:3]\n",
        "                mask = np.load(self.directory + self.mask_Namesake + str(indexes[index]) + \".npy\")\n",
        "                img, mask = self.augment_Image(img,mask)\n",
        "                X[index,:,:,:] = img\n",
        "                y_True[index,:,:,:] = mask\n",
        "        return X, y_True\n",
        "    def standard_norm(self,img):\n",
        "        height, width, channels = img.shape\n",
        "        for channel in range(channels):\n",
        "            img[:,:,channel] = (img[:,:,channel] - np.mean(img[:,:,channel]))/np.std(img[:,:,channel])\n",
        "        return img\n",
        "    def augment_Image(self, image, mask):\n",
        "        if self.rotations == True:\n",
        "            random_Integer = randint(1,5)\n",
        "            image = np.rot90(image.copy(),random_Integer)\n",
        "            mask = np.rot90(mask.copy(),random_Integer)\n",
        "        \n",
        "        if self.horizontal_Flips == True:\n",
        "            random_Float = random()\n",
        "\n",
        "            if random_Float < 0.5:\n",
        "                image = np.flip(image.copy(),0)\n",
        "                mask = np.flip(mask.copy(),0)\n",
        "        if self.vertical_Flips == True:\n",
        "            random_Float = random()\n",
        "\n",
        "            if random_Float < 0.5:\n",
        "                image = np.flip(image.copy(),1)\n",
        "                mask = np.flip(mask.copy(),1)\n",
        "        return image, mask"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_ypzBUKLrVpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can train our phase one network, using the following block of code. Note, that although we are training for 1000 epochs, that we have a condition where the training stops if no improvement is found in 10 conescutive epochs."
      ],
      "metadata": {
        "id": "5jQYAZnKtJnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Phase One Segmentation Network\n",
        "\n",
        "tile_Names_Style = \"/\" \n",
        "mask_Names_Style = \"/\"\n",
        "img_size = (128,128)\n",
        "num_classes = 2\n",
        "number_Of_Epochs = 1000\n",
        "model = Phase1_Net(img_size, num_classes)\n",
        "#learning hyperparamters for the training optimizer \n",
        "model.compile(Adam(lr=0.001),\n",
        "                 metrics = ['accuracy'],\n",
        "                 loss = tf.keras.losses.CategoricalCrossentropy())\n",
        "# model.compile(Adam(lr=0.001),\n",
        "#                 metrics = ['accuracy'],\n",
        "#                 loss = iou)\n",
        "train_Gen = DataGenerator(data_Frame=training_Data_Frame,\n",
        "                    x_Col = \"X\",\n",
        "                    y_Col = \"Y_True\",\n",
        "                    directory = flow_Directory,\n",
        "                    vertical_Flips=True,\n",
        "                    horizontal_Flips = True,\n",
        "                    rotations = True,\n",
        "                    split = True,\n",
        "                    training_Ratio = 0.8,\n",
        "                    shuffle = True,\n",
        "                    tile_Namesake = tile_Names_Style,\n",
        "                    mask_Namesake = mask_Names_Style,\n",
        "                    subset = \"Training\",\n",
        "                    number_Of_Training_Images = len(image_Names))\n",
        "validate_Gen = DataGenerator(data_Frame=training_Data_Frame,\n",
        "                    x_Col = \"X\",\n",
        "                    y_Col = \"Y_True\",\n",
        "                    directory = flow_Directory,\n",
        "                    vertical_Flips=False,\n",
        "                    horizontal_Flips = False,\n",
        "                    rotations = False,\n",
        "                    split = True,\n",
        "                    training_Ratio = 0.8,\n",
        "                    shuffle = True,\n",
        "                    tile_Namesake = tile_Names_Style,\n",
        "                    mask_Namesake = mask_Names_Style,\n",
        "                    subset = \"Validation\",\n",
        "                    number_Of_Training_Images = len(image_Names))\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto', restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"Phase_One.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "training_History = model.fit(train_Gen,\n",
        "                             validation_data = validate_Gen,\n",
        "                                 epochs=number_Of_Epochs, \n",
        "                                 verbose = 1, callbacks = [early, checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "fX7McSFXuEIM",
        "outputId": "08725ad1-aecd-406f-b511-fbeddeafc419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors for filter size:256\n",
            "No errors for filter size:512\n",
            "No errors for filter size:512\n",
            "Epoch 1/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.3920 - accuracy: 0.9620 \n",
            "Epoch 1: val_loss improved from inf to 0.14630, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 275s 22s/step - loss: 0.3920 - accuracy: 0.9620 - val_loss: 0.1463 - val_accuracy: 0.9966\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.9877 \n",
            "Epoch 2: val_loss improved from 0.14630 to 0.12432, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.3061 - accuracy: 0.9877 - val_loss: 0.1243 - val_accuracy: 0.9969\n",
            "Epoch 3/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9883 \n",
            "Epoch 3: val_loss improved from 0.12432 to 0.09746, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.2492 - accuracy: 0.9883 - val_loss: 0.0975 - val_accuracy: 0.9970\n",
            "Epoch 4/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9884 \n",
            "Epoch 4: val_loss improved from 0.09746 to 0.07630, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.1996 - accuracy: 0.9884 - val_loss: 0.0763 - val_accuracy: 0.9970\n",
            "Epoch 5/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9883 \n",
            "Epoch 5: val_loss improved from 0.07630 to 0.05996, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.1614 - accuracy: 0.9883 - val_loss: 0.0600 - val_accuracy: 0.9970\n",
            "Epoch 6/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9883 \n",
            "Epoch 6: val_loss improved from 0.05996 to 0.04923, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.1328 - accuracy: 0.9883 - val_loss: 0.0492 - val_accuracy: 0.9970\n",
            "Epoch 7/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9883 \n",
            "Epoch 7: val_loss improved from 0.04923 to 0.04113, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.1106 - accuracy: 0.9883 - val_loss: 0.0411 - val_accuracy: 0.9970\n",
            "Epoch 8/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9883 \n",
            "Epoch 8: val_loss improved from 0.04113 to 0.03548, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0933 - accuracy: 0.9883 - val_loss: 0.0355 - val_accuracy: 0.9970\n",
            "Epoch 9/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9883 \n",
            "Epoch 9: val_loss improved from 0.03548 to 0.03127, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0798 - accuracy: 0.9883 - val_loss: 0.0313 - val_accuracy: 0.9970\n",
            "Epoch 10/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9883 \n",
            "Epoch 10: val_loss improved from 0.03127 to 0.02838, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 241s 20s/step - loss: 0.0691 - accuracy: 0.9883 - val_loss: 0.0284 - val_accuracy: 0.9970\n",
            "Epoch 11/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9883 \n",
            "Epoch 11: val_loss improved from 0.02838 to 0.02638, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0606 - accuracy: 0.9883 - val_loss: 0.0264 - val_accuracy: 0.9970\n",
            "Epoch 12/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9883 \n",
            "Epoch 12: val_loss improved from 0.02638 to 0.02490, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0539 - accuracy: 0.9883 - val_loss: 0.0249 - val_accuracy: 0.9970\n",
            "Epoch 13/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9883 \n",
            "Epoch 13: val_loss improved from 0.02490 to 0.02338, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 243s 21s/step - loss: 0.0484 - accuracy: 0.9883 - val_loss: 0.0234 - val_accuracy: 0.9970\n",
            "Epoch 14/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9883 \n",
            "Epoch 14: val_loss improved from 0.02338 to 0.02217, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0439 - accuracy: 0.9883 - val_loss: 0.0222 - val_accuracy: 0.9970\n",
            "Epoch 15/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9883 \n",
            "Epoch 15: val_loss improved from 0.02217 to 0.02095, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 243s 21s/step - loss: 0.0401 - accuracy: 0.9883 - val_loss: 0.0210 - val_accuracy: 0.9970\n",
            "Epoch 16/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9883 \n",
            "Epoch 16: val_loss improved from 0.02095 to 0.01997, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.0200 - val_accuracy: 0.9970\n",
            "Epoch 17/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9883 \n",
            "Epoch 17: val_loss improved from 0.01997 to 0.01896, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0344 - accuracy: 0.9883 - val_loss: 0.0190 - val_accuracy: 0.9970\n",
            "Epoch 18/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9883 \n",
            "Epoch 18: val_loss improved from 0.01896 to 0.01840, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0323 - accuracy: 0.9883 - val_loss: 0.0184 - val_accuracy: 0.9970\n",
            "Epoch 19/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9883 \n",
            "Epoch 19: val_loss improved from 0.01840 to 0.01797, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 243s 20s/step - loss: 0.0304 - accuracy: 0.9883 - val_loss: 0.0180 - val_accuracy: 0.9970\n",
            "Epoch 20/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9883 \n",
            "Epoch 20: val_loss improved from 0.01797 to 0.01638, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0287 - accuracy: 0.9883 - val_loss: 0.0164 - val_accuracy: 0.9970\n",
            "Epoch 21/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9883 \n",
            "Epoch 21: val_loss improved from 0.01638 to 0.01613, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 241s 20s/step - loss: 0.0271 - accuracy: 0.9883 - val_loss: 0.0161 - val_accuracy: 0.9970\n",
            "Epoch 22/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9883 \n",
            "Epoch 22: val_loss improved from 0.01613 to 0.01489, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0258 - accuracy: 0.9883 - val_loss: 0.0149 - val_accuracy: 0.9970\n",
            "Epoch 23/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9883 \n",
            "Epoch 23: val_loss improved from 0.01489 to 0.01444, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 236s 20s/step - loss: 0.0246 - accuracy: 0.9883 - val_loss: 0.0144 - val_accuracy: 0.9970\n",
            "Epoch 24/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9883 \n",
            "Epoch 24: val_loss improved from 0.01444 to 0.01309, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0236 - accuracy: 0.9883 - val_loss: 0.0131 - val_accuracy: 0.9970\n",
            "Epoch 25/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9883 \n",
            "Epoch 25: val_loss improved from 0.01309 to 0.01247, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 242s 21s/step - loss: 0.0226 - accuracy: 0.9883 - val_loss: 0.0125 - val_accuracy: 0.9970\n",
            "Epoch 26/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9883 \n",
            "Epoch 26: val_loss improved from 0.01247 to 0.01203, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0218 - accuracy: 0.9883 - val_loss: 0.0120 - val_accuracy: 0.9970\n",
            "Epoch 27/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9883 \n",
            "Epoch 27: val_loss improved from 0.01203 to 0.01146, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0211 - accuracy: 0.9883 - val_loss: 0.0115 - val_accuracy: 0.9970\n",
            "Epoch 28/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9883 \n",
            "Epoch 28: val_loss improved from 0.01146 to 0.01032, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 243s 21s/step - loss: 0.0204 - accuracy: 0.9883 - val_loss: 0.0103 - val_accuracy: 0.9970\n",
            "Epoch 29/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9883 \n",
            "Epoch 29: val_loss improved from 0.01032 to 0.01024, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0199 - accuracy: 0.9883 - val_loss: 0.0102 - val_accuracy: 0.9970\n",
            "Epoch 30/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9883 \n",
            "Epoch 30: val_loss improved from 0.01024 to 0.00941, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0193 - accuracy: 0.9883 - val_loss: 0.0094 - val_accuracy: 0.9970\n",
            "Epoch 31/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9883 \n",
            "Epoch 31: val_loss improved from 0.00941 to 0.00919, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 243s 20s/step - loss: 0.0188 - accuracy: 0.9883 - val_loss: 0.0092 - val_accuracy: 0.9970\n",
            "Epoch 32/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9883 \n",
            "Epoch 32: val_loss improved from 0.00919 to 0.00854, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0183 - accuracy: 0.9883 - val_loss: 0.0085 - val_accuracy: 0.9970\n",
            "Epoch 33/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9883 \n",
            "Epoch 33: val_loss improved from 0.00854 to 0.00837, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0178 - accuracy: 0.9883 - val_loss: 0.0084 - val_accuracy: 0.9970\n",
            "Epoch 34/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9883 \n",
            "Epoch 34: val_loss improved from 0.00837 to 0.00779, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0174 - accuracy: 0.9883 - val_loss: 0.0078 - val_accuracy: 0.9970\n",
            "Epoch 35/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9883 \n",
            "Epoch 35: val_loss improved from 0.00779 to 0.00739, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0170 - accuracy: 0.9883 - val_loss: 0.0074 - val_accuracy: 0.9970\n",
            "Epoch 36/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9883 \n",
            "Epoch 36: val_loss did not improve from 0.00739\n",
            "12/12 [==============================] - 242s 21s/step - loss: 0.0167 - accuracy: 0.9883 - val_loss: 0.0074 - val_accuracy: 0.9970\n",
            "Epoch 37/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9883 \n",
            "Epoch 37: val_loss improved from 0.00739 to 0.00709, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0164 - accuracy: 0.9883 - val_loss: 0.0071 - val_accuracy: 0.9970\n",
            "Epoch 38/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9883 \n",
            "Epoch 38: val_loss improved from 0.00709 to 0.00703, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0161 - accuracy: 0.9883 - val_loss: 0.0070 - val_accuracy: 0.9970\n",
            "Epoch 39/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9883 \n",
            "Epoch 39: val_loss improved from 0.00703 to 0.00656, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0158 - accuracy: 0.9883 - val_loss: 0.0066 - val_accuracy: 0.9970\n",
            "Epoch 40/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9883 \n",
            "Epoch 40: val_loss did not improve from 0.00656\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0155 - accuracy: 0.9883 - val_loss: 0.0068 - val_accuracy: 0.9970\n",
            "Epoch 41/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9883 \n",
            "Epoch 41: val_loss improved from 0.00656 to 0.00617, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0153 - accuracy: 0.9883 - val_loss: 0.0062 - val_accuracy: 0.9970\n",
            "Epoch 42/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9883 \n",
            "Epoch 42: val_loss did not improve from 0.00617\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0151 - accuracy: 0.9883 - val_loss: 0.0063 - val_accuracy: 0.9970\n",
            "Epoch 43/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9883 \n",
            "Epoch 43: val_loss did not improve from 0.00617\n",
            "12/12 [==============================] - 241s 20s/step - loss: 0.0149 - accuracy: 0.9883 - val_loss: 0.0063 - val_accuracy: 0.9970\n",
            "Epoch 44/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9883 \n",
            "Epoch 44: val_loss improved from 0.00617 to 0.00588, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0147 - accuracy: 0.9883 - val_loss: 0.0059 - val_accuracy: 0.9970\n",
            "Epoch 45/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9883 \n",
            "Epoch 45: val_loss did not improve from 0.00588\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0145 - accuracy: 0.9883 - val_loss: 0.0061 - val_accuracy: 0.9970\n",
            "Epoch 46/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9883 \n",
            "Epoch 46: val_loss improved from 0.00588 to 0.00586, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0143 - accuracy: 0.9883 - val_loss: 0.0059 - val_accuracy: 0.9970\n",
            "Epoch 47/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9883 \n",
            "Epoch 47: val_loss improved from 0.00586 to 0.00572, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0141 - accuracy: 0.9883 - val_loss: 0.0057 - val_accuracy: 0.9970\n",
            "Epoch 48/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9883 \n",
            "Epoch 48: val_loss did not improve from 0.00572\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0139 - accuracy: 0.9883 - val_loss: 0.0059 - val_accuracy: 0.9970\n",
            "Epoch 49/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9883 \n",
            "Epoch 49: val_loss did not improve from 0.00572\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0137 - accuracy: 0.9883 - val_loss: 0.0058 - val_accuracy: 0.9970\n",
            "Epoch 50/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9883 \n",
            "Epoch 50: val_loss did not improve from 0.00572\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0135 - accuracy: 0.9883 - val_loss: 0.0058 - val_accuracy: 0.9970\n",
            "Epoch 51/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9883 \n",
            "Epoch 51: val_loss improved from 0.00572 to 0.00565, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 241s 20s/step - loss: 0.0134 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 52/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9883 \n",
            "Epoch 52: val_loss did not improve from 0.00565\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0132 - accuracy: 0.9883 - val_loss: 0.0057 - val_accuracy: 0.9970\n",
            "Epoch 53/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9883 \n",
            "Epoch 53: val_loss improved from 0.00565 to 0.00564, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 241s 20s/step - loss: 0.0131 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 54/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9883 \n",
            "Epoch 54: val_loss improved from 0.00564 to 0.00560, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 242s 20s/step - loss: 0.0130 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 55/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9883 \n",
            "Epoch 55: val_loss did not improve from 0.00560\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0129 - accuracy: 0.9883 - val_loss: 0.0060 - val_accuracy: 0.9970\n",
            "Epoch 56/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9883 \n",
            "Epoch 56: val_loss improved from 0.00560 to 0.00553, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 242s 21s/step - loss: 0.0128 - accuracy: 0.9883 - val_loss: 0.0055 - val_accuracy: 0.9970\n",
            "Epoch 57/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0127 - accuracy: 0.9883 \n",
            "Epoch 57: val_loss did not improve from 0.00553\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0127 - accuracy: 0.9883 - val_loss: 0.0057 - val_accuracy: 0.9970\n",
            "Epoch 58/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9883 \n",
            "Epoch 58: val_loss did not improve from 0.00553\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0125 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 59/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9883 \n",
            "Epoch 59: val_loss did not improve from 0.00553\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0124 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 60/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9883 \n",
            "Epoch 60: val_loss did not improve from 0.00553\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0123 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 61/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9883 \n",
            "Epoch 61: val_loss did not improve from 0.00553\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0122 - accuracy: 0.9883 - val_loss: 0.0058 - val_accuracy: 0.9970\n",
            "Epoch 62/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9883 \n",
            "Epoch 62: val_loss did not improve from 0.00553\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0121 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 63/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9883 \n",
            "Epoch 63: val_loss improved from 0.00553 to 0.00543, saving model to Phase_One.h5\n",
            "12/12 [==============================] - 239s 20s/step - loss: 0.0119 - accuracy: 0.9883 - val_loss: 0.0054 - val_accuracy: 0.9970\n",
            "Epoch 64/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9883 \n",
            "Epoch 64: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0118 - accuracy: 0.9883 - val_loss: 0.0057 - val_accuracy: 0.9970\n",
            "Epoch 65/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9883 \n",
            "Epoch 65: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0116 - accuracy: 0.9883 - val_loss: 0.0058 - val_accuracy: 0.9970\n",
            "Epoch 66/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9883 \n",
            "Epoch 66: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0115 - accuracy: 0.9883 - val_loss: 0.0057 - val_accuracy: 0.9970\n",
            "Epoch 67/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9883 \n",
            "Epoch 67: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0114 - accuracy: 0.9883 - val_loss: 0.0059 - val_accuracy: 0.9970\n",
            "Epoch 68/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9883 \n",
            "Epoch 68: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 238s 20s/step - loss: 0.0113 - accuracy: 0.9883 - val_loss: 0.0060 - val_accuracy: 0.9970\n",
            "Epoch 69/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9883 \n",
            "Epoch 69: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 240s 20s/step - loss: 0.0113 - accuracy: 0.9883 - val_loss: 0.0056 - val_accuracy: 0.9970\n",
            "Epoch 70/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9883 \n",
            "Epoch 70: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0112 - accuracy: 0.9883 - val_loss: 0.0067 - val_accuracy: 0.9970\n",
            "Epoch 71/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9883 \n",
            "Epoch 71: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0112 - accuracy: 0.9883 - val_loss: 0.0068 - val_accuracy: 0.9970\n",
            "Epoch 72/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9883 \n",
            "Epoch 72: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 237s 20s/step - loss: 0.0112 - accuracy: 0.9883 - val_loss: 0.0081 - val_accuracy: 0.9970\n",
            "Epoch 73/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9883 Restoring model weights from the end of the best epoch: 63.\n",
            "\n",
            "Epoch 73: val_loss did not improve from 0.00543\n",
            "12/12 [==============================] - 236s 20s/step - loss: 0.0111 - accuracy: 0.9883 - val_loss: 0.0094 - val_accuracy: 0.9970\n",
            "Epoch 73: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "a4fPngVhmZcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Model\n",
        "model.save(base_Directory + \"/Phase_One_Model/Phase_One_Trained_Network.h5\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cpkIqtbdmYzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained network will be saved to the location **motion-blur-microscopy -> Training_Material -> Train_Phase_One -> Phase_One_Model**."
      ],
      "metadata": {
        "id": "mbqoEA-o2fJB"
      }
    }
  ]
}