{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46256240",
   "metadata": {},
   "source": [
    "#### Run the following chunk of code to import all of the necessary libraries and packages to run the rest of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "listed-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Conv2D, MaxPooling2D, Input, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os as os\n",
    "import shutil\n",
    "import cv2 as cv2\n",
    "\n",
    "from random import randint\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9f860",
   "metadata": {},
   "source": [
    "#### Run the following chunk of code to move all original tiles and the one-hot encoded truth arrays to a \"flow folder\". The script will also generate a .csv file, containing file names of images, as well as file names of one-hot encoded truth arrays.\n",
    "\n",
    "Users will have to adjust the base_Directory variable for their own use. Please note the file layout convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "configured-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_Directory = \"\"\n",
    "image_Names = os.listdir(base_Directory + \"Raw_Training_Tiles_Resized/\")\n",
    "mask_Names = os.listdir(base_Directory + \"Training_Tiles_Labeled_And_Layered_Resized/\")\n",
    "flow_Directory = base_Directory + \"Flow_Folder\"\n",
    "data_Frame_Location = base_Directory + \"Excel_Directory/Training_File_Names.csv\"\n",
    "\n",
    "dict = {'X': image_Names, 'Y_True': mask_Names} \n",
    "training_Data_Frame = pd.DataFrame(dict)\n",
    "training_Data_Frame.to_csv(data_Frame_Location)\n",
    "\n",
    "for file in os.listdir(base_Directory + \"Raw_Training_Tiles_Resized/\"):\n",
    "  shutil.copy(base_Directory + \"Raw_Training_Tiles_Resized/\" + file, base_Directory + \"Flow_Folder/\" + file)\n",
    "\n",
    "for file in os.listdir(base_Directory + \"Training_Tiles_Labeled_And_Layered_Resized/\"):\n",
    "  shutil.copy(base_Directory + \"Training_Tiles_Labeled_And_Layered_Resized/\" + file, base_Directory + \"Flow_Folder/\" + file)\n",
    "\n",
    "dict = {'X': np.sort(image_Names), 'Y_True': np.sort(mask_Names)} \n",
    "training_Data_Frame = pd.DataFrame(dict)\n",
    "training_Data_Frame.to_csv(data_Frame_Location)\n",
    "\n",
    "\n",
    "\n",
    "training_Data_Frame = pd.read_csv(data_Frame_Location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c79bf1",
   "metadata": {},
   "source": [
    "#### Run the following chunk of code to generate the machine learning network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Phase 1 Models ====================\n",
    "\n",
    "def Phase1_Net(img_size, num_classes):\n",
    "    inputs = Input(shape=img_size + (3,))\n",
    "\n",
    "    x = Conv2D(64,kernel_size = 3, strides = (1,1),\n",
    "                            padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    \n",
    "    previous_block_concatenate1 = x\n",
    "    x = MaxPooling2D(pool_size = (2,2),\n",
    "                                  strides = (2,2))(x)\n",
    "\n",
    "    x = Conv2D(128,kernel_size = 3, strides = (1,1),\n",
    "                            padding = \"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2),\n",
    "                                  strides = (2,2))(x)\n",
    "\n",
    "    previous_block_concatenate2 = x\n",
    "\n",
    "    concate_block_num = 3\n",
    "    for filters in [256, 512, 512]:\n",
    "        x = Conv2D(filters,3, strides = (1,1),\n",
    "                            padding = \"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activations.relu)(x)\n",
    "        x = Conv2D(filters,3, strides = 1,\n",
    "                         padding = \"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activations.relu)(x)\n",
    "        x = MaxPooling2D(pool_size = (2,2),\n",
    "                                  strides = (2,2))(x)\n",
    "        globals()['previous_block_concatenate%s' % concate_block_num] = x\n",
    "        concate_block_num = concate_block_num + 1\n",
    "        print((\"No errors for filter size:\" + str(filters)))\n",
    "\n",
    "\n",
    "\n",
    "    x = Conv2D(512,3, strides = 1,\n",
    "                            padding = \"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = MaxPooling2D(pool_size = (2,2),\n",
    "                                  strides = (2,2))(x)\n",
    "\n",
    "    x = Conv2D(512,3, strides = 1,\n",
    "                            padding = \"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    x = Conv2DTranspose(256,2, strides = (2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    x = concatenate([x, previous_block_concatenate5], axis =-1)\n",
    "\n",
    "    x = Conv2DTranspose(256,2, strides = (2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    x = concatenate([x, previous_block_concatenate4],axis=-1)\n",
    "\n",
    "    x = Conv2DTranspose(128,2, strides = (2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    x = concatenate([x, previous_block_concatenate3],axis=-1)\n",
    "    \n",
    "    x = Conv2DTranspose(64,2, strides = (2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    x = concatenate([x, previous_block_concatenate2],axis=-1)\n",
    "\n",
    "\n",
    "    x = Conv2DTranspose(32,2, strides = (2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "    x = Conv2DTranspose(64,2, strides = (2,2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "\n",
    "\n",
    "    x = concatenate([x, previous_block_concatenate1],axis=-1)\n",
    "\n",
    "    x = Conv2D(32,3, strides = (1,1),\n",
    "                            padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    x = Conv2D(num_classes,3, strides = (1,1),\n",
    "                            padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activations.relu)(x)\n",
    "    outputs = Conv2D(num_classes,3, strides = (1,1),\n",
    "                            activation = 'softmax',\n",
    "                            padding = 'same',\n",
    "                            name = 'sRBC_classes')(x)\n",
    "    model = Model(inputs,outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# ================ Train Phase 2 Model ================\n",
    "\n",
    "# training the model for a specific amount of epochs \n",
    "def Phase1_train_network(model, X_train, y_train, \n",
    "                        X_test, y_test, epochs):\n",
    "    \n",
    "    train_history = model.fit(X_train, y_train, epochs=epochs, \n",
    "                              validation_data=(X_test, y_test),\n",
    "                              shuffle = True, verbose = 2)\n",
    "    return model, train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabad139",
   "metadata": {},
   "source": [
    "#### Run the following chunk of code to create a data generator, which will read tiles into the training method in batches, with augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nominated-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data_Frame, x_Col, y_Col, directory,tile_Namesake, mask_Namesake, subset = None,\n",
    "                 horizontal_Flips = False, vertical_Flips = False, rotations = False, batch_Size = 32,\n",
    "                 split = False, training_Ratio = 1, shuffle = False, dim = (128,128), number_Of_Channels = 3,\n",
    "                 number_Of_Classes = 2, sample_Mean_Zero_Center_Standarardization = True, number_Of_Training_Images = None):\n",
    "        self.batch_Size = batch_Size\n",
    "        self.df = data_Frame\n",
    "        self.x_Col = x_Col\n",
    "        self.y_Col = y_Col\n",
    "        self.dim = dim\n",
    "        self.directory = directory\n",
    "        self.subset = subset\n",
    "        self.sample_Mean_Zero_Center_Standarardization = sample_Mean_Zero_Center_Standarardization\n",
    "        self.number_Of_Classes = number_Of_Classes\n",
    "        self.number_Of_Channels = number_Of_Channels\n",
    "        self.shuffle = shuffle\n",
    "        self.tile_Names = self.df[self.x_Col]\n",
    "        self.truth_Names = self.df[self.y_Col]\n",
    "        self.tile_Namesake = tile_Namesake\n",
    "        self.mask_Namesake = mask_Namesake\n",
    "        self.horizontal_Flips = horizontal_Flips\n",
    "        self.vertical_Flips = vertical_Flips\n",
    "        self.number_Of_Training_Images = number_Of_Training_Images\n",
    "        self.index_List = np.arange(number_Of_Training_Images) + 1\n",
    "        self.rotations = rotations\n",
    "        self.training_Samples = int(training_Ratio*len(self.index_List))\n",
    "        if split == True:\n",
    "            self.train_Index_List = self.index_List[:self.training_Samples]\n",
    "            self.validate_Index_List = self.index_List[self.training_Samples:]\n",
    "        else:\n",
    "            self.train_Index_List = self.index_List[:]\n",
    "            self.validate_Index_List = []\n",
    "        if self.shuffle == True:\n",
    "            self.on_Epoch_End()\n",
    "    def __len__(self):\n",
    "        return int(len(self.train_Index_List)/self.batch_Size)\n",
    "    def __getitem__(self, index):\n",
    "        if self.subset == \"Training\":\n",
    "            indexes = self.train_Index_List[index*self.batch_Size:(index*self.batch_Size) + self.batch_Size]\n",
    "            X, y_True = self.generate_Batch(indexes)\n",
    "        elif self.subset == \"Validation\":\n",
    "            indexes = self.validate_Index_List[index*self.batch_Size:(index*self.batch_Size) + self.batch_Size]\n",
    "            X, y_True = self.generate_Batch(indexes)\n",
    "        else:\n",
    "            indexes = self.train_Index_List[index*self.batch_Size:(index*self.batch_Size) + self.batch_Size]\n",
    "            X, y_True = self.generate_Batch(indexes)\n",
    "        return X, y_True\n",
    "    def on_Epoch_End(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.train_Index_List)\n",
    "    def generate_Batch(self,indexes):\n",
    "        X = np.zeros((self.batch_Size, *self.dim, self.number_Of_Channels))\n",
    "        y_True = np.zeros((self.batch_Size, *self.dim, self.number_Of_Classes))\n",
    "        for index in range(len(indexes)):\n",
    "            if self.sample_Mean_Zero_Center_Standarardization == True:\n",
    "                img = plt.imread(self.directory + self.tile_Namesake + str(indexes[index]) + \".png\")[:,:,0:3]\n",
    "                if np.max(img) == int(np.max(img)) and len(str(np.max(img))) == len(str(int(np.max(img)))):\n",
    "                    img = img.copy()/255.\n",
    "                if len(np.shape(img)) == 2:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                if np.shape(img)[2] == 4:\n",
    "                    img = img.copy()[:,:,0:3]\n",
    "                mask = np.load(self.directory + self.mask_Namesake + str(indexes[index]) + \".npy\")\n",
    "                img, mask = self.augment_Image(img,mask)\n",
    "                X[index,:,:,:] = self.standard_norm(plt.imread(self.directory + self.tile_Namesake + str(indexes[index]) + \".png\")[:,:,0:3])\n",
    "                y_True[index,:,:,:] = np.load(self.directory + self.mask_Namesake + str(indexes[index]) + \".npy\")\n",
    "            else:\n",
    "                img = plt.imread(self.directory + self.tile_Namesake + str(indexes[index]) + \".png\")[:,:,0:3]\n",
    "                if np.max(img) == int(np.max(img)) and len(str(np.max(img))) == len(str(int(np.max(img)))):\n",
    "                    img = img.copy()/255.\n",
    "                if len(np.shape(img)) == 2:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                if np.shape(img)[2] == 4:\n",
    "                    img = img.copy()[:,:,0:3]\n",
    "                mask = np.load(self.directory + self.mask_Namesake + str(indexes[index]) + \".npy\")\n",
    "                img, mask = self.augment_Image(img,mask)\n",
    "                X[index,:,:,:] = img\n",
    "                y_True[index,:,:,:] = mask\n",
    "        return X, y_True\n",
    "    def standard_norm(self,img):\n",
    "        height, width, channels = img.shape\n",
    "        for channel in range(channels):\n",
    "            img[:,:,channel] = (img[:,:,channel] - np.mean(img[:,:,channel]))/np.std(img[:,:,channel])\n",
    "        return img\n",
    "    def augment_Image(self, image, mask):\n",
    "        if self.rotations == True:\n",
    "            random_Integer = randint(1,5)\n",
    "            image = np.rot90(image.copy(),random_Integer)\n",
    "            mask = np.rot90(mask.copy(),random_Integer)\n",
    "        \n",
    "        if self.horizontal_Flips == True:\n",
    "            random_Float = random()\n",
    "\n",
    "            if random_Float < 0.5:\n",
    "                image = np.flip(image.copy(),0)\n",
    "                mask = np.flip(mask.copy(),0)\n",
    "        if self.vertical_Flips == True:\n",
    "            random_Float = random()\n",
    "\n",
    "            if random_Float < 0.5:\n",
    "                image = np.flip(image.copy(),1)\n",
    "                mask = np.flip(mask.copy(),1)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db3871",
   "metadata": {},
   "source": [
    "#### Run the following chunk of code to train the semantic segmantation machine learning architecture.\n",
    "Note that the number of epochs is intentially made large. Users are not intended to reach the number_Of_Epochs variable. Instead, the learning will be limited by early stopping, given by the patience variable. This variable gives the number of epochs without improvement by the network before training is automatically stopped. Users may need to change the tile_Names_Style and mask_Names_Style variables to match their naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chemical-charter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors for filter size:256\n",
      "No errors for filter size:512\n",
      "No errors for filter size:512\n",
      "Epoch 1/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.9407\n",
      "Epoch 1: val_loss improved from inf to 0.13796, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.3485 - accuracy: 0.9407 - val_loss: 0.1380 - val_accuracy: 0.9908\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9641\n",
      "Epoch 2: val_loss improved from 0.13796 to 0.08375, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.2332 - accuracy: 0.9641 - val_loss: 0.0838 - val_accuracy: 0.9914\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9646\n",
      "Epoch 3: val_loss improved from 0.08375 to 0.05160, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.1746 - accuracy: 0.9646 - val_loss: 0.0516 - val_accuracy: 0.9914\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9657\n",
      "Epoch 4: val_loss improved from 0.05160 to 0.03566, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.1367 - accuracy: 0.9657 - val_loss: 0.0357 - val_accuracy: 0.9914\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9658\n",
      "Epoch 5: val_loss improved from 0.03566 to 0.03114, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 66s 4s/step - loss: 0.1116 - accuracy: 0.9658 - val_loss: 0.0311 - val_accuracy: 0.9914\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9658\n",
      "Epoch 6: val_loss improved from 0.03114 to 0.02831, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.0949 - accuracy: 0.9658 - val_loss: 0.0283 - val_accuracy: 0.9914\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9658\n",
      "Epoch 7: val_loss improved from 0.02831 to 0.02742, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0828 - accuracy: 0.9658 - val_loss: 0.0274 - val_accuracy: 0.9914\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9658\n",
      "Epoch 8: val_loss improved from 0.02742 to 0.02572, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.0746 - accuracy: 0.9658 - val_loss: 0.0257 - val_accuracy: 0.9914\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9658\n",
      "Epoch 9: val_loss did not improve from 0.02572\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.0679 - accuracy: 0.9658 - val_loss: 0.0277 - val_accuracy: 0.9914\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9658\n",
      "Epoch 10: val_loss improved from 0.02572 to 0.02309, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.0630 - accuracy: 0.9658 - val_loss: 0.0231 - val_accuracy: 0.9914\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9658\n",
      "Epoch 11: val_loss did not improve from 0.02309\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.0593 - accuracy: 0.9658 - val_loss: 0.0303 - val_accuracy: 0.9914\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9658\n",
      "Epoch 12: val_loss did not improve from 0.02309\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0562 - accuracy: 0.9658 - val_loss: 0.0235 - val_accuracy: 0.9914\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9658\n",
      "Epoch 13: val_loss improved from 0.02309 to 0.02209, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 59s 4s/step - loss: 0.0531 - accuracy: 0.9658 - val_loss: 0.0221 - val_accuracy: 0.9914\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9658\n",
      "Epoch 14: val_loss improved from 0.02209 to 0.01916, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.0506 - accuracy: 0.9658 - val_loss: 0.0192 - val_accuracy: 0.9914\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9658\n",
      "Epoch 15: val_loss improved from 0.01916 to 0.01885, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0486 - accuracy: 0.9658 - val_loss: 0.0188 - val_accuracy: 0.9914\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9658\n",
      "Epoch 16: val_loss did not improve from 0.01885\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.0469 - accuracy: 0.9658 - val_loss: 0.0208 - val_accuracy: 0.9914\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9658\n",
      "Epoch 17: val_loss improved from 0.01885 to 0.01616, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 60s 4s/step - loss: 0.0453 - accuracy: 0.9658 - val_loss: 0.0162 - val_accuracy: 0.9914\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9658\n",
      "Epoch 18: val_loss did not improve from 0.01616\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0443 - accuracy: 0.9658 - val_loss: 0.0188 - val_accuracy: 0.9914\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9658\n",
      "Epoch 19: val_loss did not improve from 0.01616\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.0437 - accuracy: 0.9658 - val_loss: 0.0175 - val_accuracy: 0.9914\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9658\n",
      "Epoch 20: val_loss improved from 0.01616 to 0.01604, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0424 - accuracy: 0.9658 - val_loss: 0.0160 - val_accuracy: 0.9914\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9658\n",
      "Epoch 21: val_loss did not improve from 0.01604\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0413 - accuracy: 0.9658 - val_loss: 0.0167 - val_accuracy: 0.9914\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9658\n",
      "Epoch 22: val_loss improved from 0.01604 to 0.01481, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 61s 4s/step - loss: 0.0400 - accuracy: 0.9658 - val_loss: 0.0148 - val_accuracy: 0.9914\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9658\n",
      "Epoch 23: val_loss did not improve from 0.01481\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.0391 - accuracy: 0.9658 - val_loss: 0.0157 - val_accuracy: 0.9914\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9658\n",
      "Epoch 24: val_loss improved from 0.01481 to 0.01390, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0384 - accuracy: 0.9658 - val_loss: 0.0139 - val_accuracy: 0.9914\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9658\n",
      "Epoch 25: val_loss improved from 0.01390 to 0.01380, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.0375 - accuracy: 0.9658 - val_loss: 0.0138 - val_accuracy: 0.9914\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9658\n",
      "Epoch 26: val_loss did not improve from 0.01380\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0369 - accuracy: 0.9658 - val_loss: 0.0149 - val_accuracy: 0.9914\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9658\n",
      "Epoch 27: val_loss did not improve from 0.01380\n",
      "17/17 [==============================] - 63s 4s/step - loss: 0.0365 - accuracy: 0.9658 - val_loss: 0.0139 - val_accuracy: 0.9914\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9658\n",
      "Epoch 28: val_loss did not improve from 0.01380\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0359 - accuracy: 0.9658 - val_loss: 0.0141 - val_accuracy: 0.9914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9658\n",
      "Epoch 29: val_loss did not improve from 0.01380\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0351 - accuracy: 0.9658 - val_loss: 0.0140 - val_accuracy: 0.9914\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 0.9658\n",
      "Epoch 30: val_loss did not improve from 0.01380\n",
      "17/17 [==============================] - 66s 4s/step - loss: 0.0346 - accuracy: 0.9658 - val_loss: 0.0139 - val_accuracy: 0.9914\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9658\n",
      "Epoch 31: val_loss improved from 0.01380 to 0.01368, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 66s 4s/step - loss: 0.0340 - accuracy: 0.9658 - val_loss: 0.0137 - val_accuracy: 0.9914\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9658\n",
      "Epoch 32: val_loss did not improve from 0.01368\n",
      "17/17 [==============================] - 66s 4s/step - loss: 0.0335 - accuracy: 0.9658 - val_loss: 0.0139 - val_accuracy: 0.9914\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9658\n",
      "Epoch 33: val_loss improved from 0.01368 to 0.01347, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 66s 4s/step - loss: 0.0332 - accuracy: 0.9658 - val_loss: 0.0135 - val_accuracy: 0.9914\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9658\n",
      "Epoch 34: val_loss did not improve from 0.01347\n",
      "17/17 [==============================] - 65s 4s/step - loss: 0.0328 - accuracy: 0.9658 - val_loss: 0.0140 - val_accuracy: 0.9914\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9658\n",
      "Epoch 35: val_loss did not improve from 0.01347\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.0322 - accuracy: 0.9658 - val_loss: 0.0137 - val_accuracy: 0.9914\n",
      "Epoch 36/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9658\n",
      "Epoch 36: val_loss improved from 0.01347 to 0.01345, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 66s 4s/step - loss: 0.0317 - accuracy: 0.9658 - val_loss: 0.0135 - val_accuracy: 0.9914\n",
      "Epoch 37/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9658\n",
      "Epoch 37: val_loss improved from 0.01345 to 0.01341, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 64s 4s/step - loss: 0.0314 - accuracy: 0.9658 - val_loss: 0.0134 - val_accuracy: 0.9914\n",
      "Epoch 38/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9658\n",
      "Epoch 38: val_loss did not improve from 0.01341\n",
      "17/17 [==============================] - 62s 4s/step - loss: 0.0310 - accuracy: 0.9658 - val_loss: 0.0141 - val_accuracy: 0.9914\n",
      "Epoch 39/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9658\n",
      "Epoch 39: val_loss did not improve from 0.01341\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.0308 - accuracy: 0.9658 - val_loss: 0.0141 - val_accuracy: 0.9914\n",
      "Epoch 40/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9658\n",
      "Epoch 40: val_loss improved from 0.01341 to 0.01323, saving model to Phase_One.h5\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0304 - accuracy: 0.9658 - val_loss: 0.0132 - val_accuracy: 0.9914\n",
      "Epoch 41/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9658\n",
      "Epoch 41: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0301 - accuracy: 0.9658 - val_loss: 0.0143 - val_accuracy: 0.9964\n",
      "Epoch 42/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9942\n",
      "Epoch 42: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0297 - accuracy: 0.9942 - val_loss: 0.0135 - val_accuracy: 0.9966\n",
      "Epoch 43/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9944\n",
      "Epoch 43: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0295 - accuracy: 0.9944 - val_loss: 0.0155 - val_accuracy: 0.9962\n",
      "Epoch 44/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9946\n",
      "Epoch 44: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0293 - accuracy: 0.9946 - val_loss: 0.0148 - val_accuracy: 0.9963\n",
      "Epoch 45/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9948\n",
      "Epoch 45: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0289 - accuracy: 0.9948 - val_loss: 0.0157 - val_accuracy: 0.9963\n",
      "Epoch 46/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9949\n",
      "Epoch 46: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0286 - accuracy: 0.9949 - val_loss: 0.0156 - val_accuracy: 0.9963\n",
      "Epoch 47/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9950\n",
      "Epoch 47: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0284 - accuracy: 0.9950 - val_loss: 0.0148 - val_accuracy: 0.9964\n",
      "Epoch 48/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9952\n",
      "Epoch 48: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0279 - accuracy: 0.9952 - val_loss: 0.0140 - val_accuracy: 0.9966\n",
      "Epoch 49/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9954\n",
      "Epoch 49: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0274 - accuracy: 0.9954 - val_loss: 0.0153 - val_accuracy: 0.9964\n",
      "Epoch 50/1000\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9955Restoring model weights from the end of the best epoch: 40.\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.01323\n",
      "17/17 [==============================] - 55s 3s/step - loss: 0.0270 - accuracy: 0.9955 - val_loss: 0.0144 - val_accuracy: 0.9966\n",
      "Epoch 50: early stopping\n"
     ]
    }
   ],
   "source": [
    "tile_Names_Style = \"/\" \n",
    "mask_Names_Style = \"/\"\n",
    "img_size = (128,128)\n",
    "num_classes = 2\n",
    "number_Of_Epochs = 1000\n",
    "model = Phase1_Net(img_size, num_classes)\n",
    "#learning hyperparamters for the training optimizer \n",
    "model.compile(Adam(learning_rate=0.001),\n",
    "                 metrics = ['accuracy'],\n",
    "                 loss = tf.keras.losses.CategoricalCrossentropy())\n",
    "train_Gen = DataGenerator(data_Frame=training_Data_Frame,\n",
    "                    x_Col = \"X\",\n",
    "                    y_Col = \"Y_True\",\n",
    "                    directory = flow_Directory,\n",
    "                    vertical_Flips=True,\n",
    "                    horizontal_Flips = True,\n",
    "                    rotations = True,\n",
    "                    split = True,\n",
    "                    training_Ratio = 0.8,\n",
    "                    shuffle = True,\n",
    "                    tile_Namesake = tile_Names_Style,\n",
    "                    mask_Namesake = mask_Names_Style,\n",
    "                    subset = \"Training\",\n",
    "                    number_Of_Training_Images = len(image_Names))\n",
    "validate_Gen = DataGenerator(data_Frame=training_Data_Frame,\n",
    "                    x_Col = \"X\",\n",
    "                    y_Col = \"Y_True\",\n",
    "                    directory = flow_Directory,\n",
    "                    vertical_Flips=False,\n",
    "                    horizontal_Flips = False,\n",
    "                    rotations = False,\n",
    "                    split = True,\n",
    "                    training_Ratio = 0.8,\n",
    "                    shuffle = True,\n",
    "                    tile_Namesake = tile_Names_Style,\n",
    "                    mask_Namesake = mask_Names_Style,\n",
    "                    subset = \"Validation\",\n",
    "                    number_Of_Training_Images = len(image_Names))\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto', restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"Phase_One.h5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "training_History = model.fit(train_Gen,\n",
    "                             validation_data = validate_Gen,\n",
    "                                 epochs=number_Of_Epochs, \n",
    "                                 verbose = 1, callbacks = [early, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb37fff",
   "metadata": {},
   "source": [
    "#### Run the following chunk of code to save the trained network architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deluxe-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(base_Directory + \"/Trained_Model/Phase_One_Trained_Network.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
